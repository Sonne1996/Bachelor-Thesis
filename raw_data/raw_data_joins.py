import sqlite3
import pandas as pd
import datetime
import os

def create_anonymous_sqlite(source_db='raw_not_anomyzed.sqlite', target_db='raw.sqlite'):
    """
    Erstellt eine neue SQLite-Datenbank und kopiert nur die relevanten Tabellen.
    """
    print(f"Starte Anonymisierung: {source_db} -> {target_db}...")
    
    # Verbindung zur Quell- und Zieldatenbank herstellen
    src_conn = sqlite3.connect(source_db)
    tgt_conn = sqlite3.connect(target_db)
    
    # Liste der Tabellen, die übernommen werden sollen
    allowed_tables = [
        'shared_members',
        'shared_member_answers',
        'shared_member_gradings',
        'shared_grading_feedback'
    ]
    
    try:
        for table in allowed_tables:
            print(f"Kopiere Tabelle: {table}...")
            # Gesamte Tabelle in ein DataFrame laden
            df = pd.read_sql_query(f"SELECT * FROM {table}", src_conn)
            
            # In die neue Datenbank schreiben
            # if_exists='replace' sorgt dafür, dass die Datei bei jedem Run neu erstellt wird
            df.to_sql(table, tgt_conn, index=False, if_exists='replace')
            
        print("Anonymisierung erfolgreich abgeschlossen.")
        
    except Exception as e:
        print(f"Fehler bei der Anonymisierung: {e}")
        
    finally:
        src_conn.close()
        tgt_conn.close()

def try_anonymize_data():
    source_db = 'raw_not_anomyzed.sqlite'
    target_db = 'raw.sqlite'
    
    # Prüfen, ob die Quell-Datenbank existiert
    if os.path.exists(source_db):
        print(f"Quell-Datei '{source_db}' gefunden. Starte Anonymisierung...")
        create_anonymous_sqlite(source_db, target_db)
    else:
        print(f"Hinweis: '{source_db}' nicht gefunden. Überspringe Anonymisierung.")
        
    # Prüfen, ob die Arbeits-Datenbank (raw.sqlite) existiert, um fortzufahren
    if not os.path.exists(target_db):
        print(f"Fehler: '{target_db}' existiert nicht. Skript wird abgebrochen.")
        return

def open_connection():
    conn = sqlite3.connect('raw.sqlite')
    return conn

def close_connection(conn):
    conn.close()

def save_to_parquet(file_name, df):
    target_file = f"{file_name}.parquet"

    df.to_parquet(target_file, engine='pyarrow', compression='snappy', index=False)
    print(f"Parquet-Datei erfolgreich erstellt: {target_file}")
    
    check_df = pd.read_parquet(target_file)
    print(f"Validierung: {len(check_df)} Zeilen in {target_file} gefunden.")

def file_information(file_name, df, join_stats=None):
    info_file = f"{file_name}_metadata.txt"
    with open(info_file, 'w', encoding='utf-8') as f:
        f.write(f"REPORT: METADATA FOR {file_name.upper()}\n")
        f.write("="*40 + "\n")
        f.write(f"Erstellt am:     {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Einträge im Resultat (Rows): {len(df)}\n")
        f.write(f"Spalten (Cols):  {len(df.columns)}\n")
        
        if join_stats:
            f.write("-" * 40 + "\n")
            f.write("TABELLENGRÖSSEN & JOIN-ANALYSE:\n")
            for key, value in join_stats.items():
                # Formatiert die Ausgabe: Tabellennamen oder Statusmeldungen
                f.write(f"- {key:<35}: {value}\n")
        
        f.write("-" * 40 + "\n")
        f.write("SPALTEN-DETAILS:\n")
        for col in df.columns:
            null_count = df[col].isnull().sum()
            dtype = df[col].dtype
            f.write(f"- {col:<40} | Typ: {str(dtype):<10} | Missing: {null_count}\n")
            
        f.write("-" * 40 + "\n")
        f.write(f"Speicherverbrauch im RAM: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n")
        f.write("="*40 + "\n")
    print(f"Metadaten-Report erstellt: {info_file}")

def get_table_size(conn, table_name):
    """Gibt die Anzahl der Zeilen einer Tabelle zurück."""
    query = f"SELECT COUNT(*) FROM {table_name}"
    return pd.read_sql_query(query, conn).iloc[0, 0]

def get_lost_rows_count(conn, base_table, join_table, base_key, join_key):
    """Zählt IDs, die in der Ziel-Tabelle fehlen."""
    query = f"""
    SELECT COUNT(*) 
    FROM {base_table} 
    WHERE {base_key} NOT IN (SELECT {join_key} FROM {join_table} WHERE {join_key} IS NOT NULL)
    """
    return pd.read_sql_query(query, conn).iloc[0, 0]

def fallback(p_name, df, stats):
    if not df.empty:
            save_to_parquet(p_name, df)
            file_information(p_name, df, join_stats=stats)
    else:
        print("Warnung: Join ergab keine Daten!")


# Joins shared_members with shared_members_answers
# Original rows of shared_members / (kept/not kept):
#   - ID / kept as member_id
#   - name / kept
#   - subject_id / kept
# Original rows of shared_members_answers / (kept/not kept):
#   - question_id / kept
#   - content / kept as answer
#   - question / kept
def join_one(conn):
    stats = {
        "Größe 'shared_members'": get_table_size(conn, "shared_members"),
        "Größe 'shared_member_answers'": get_table_size(conn, "shared_member_answers"),
        "Mitglieder ohne Antworten": get_lost_rows_count(conn, "shared_members", "shared_member_answers", "id", "shared_member_id")
    }
    
    query = """
    SELECT  m.id AS member_id, 
            m.name, 
            m.subject_id, 
            a.id AS answer_id,
            a.question_id, 
            a.content AS answer, 
            a.question 
    FROM shared_members m 
    JOIN shared_member_answers a ON m.id = a.shared_member_id
    """
    return pd.read_sql_query(query, conn), stats

def join_two(conn):
    stats = {
        "Größe 'shared_member_answers'": get_table_size(conn, "shared_member_answers"),
        "Größe 'shared_member_gradings'": get_table_size(conn, "shared_member_gradings"),
        "Antworten ohne KI-Grading": get_lost_rows_count(conn, "shared_member_answers", "shared_member_gradings", "id", "answer_id")
    }
    
    query = """
    SELECT  m.id AS member_id, 
            m.name, 
            m.subject_id, 
            a.id AS answer_id,
            a.question_id, 
            a.content AS answer, 
            a.question,
            g.accuracy, 
            g.content AS model_prediction, 
            g.used_model, 
            g.used_rubric, 
            g.used_examples,
            g.response AS model_response_with_metadata
    FROM shared_members m
    JOIN shared_member_answers a ON m.id = a.shared_member_id
    JOIN shared_member_gradings g ON a.id = g.answer_id
    """
    return pd.read_sql_query(query, conn), stats

def join_three(conn):

    stats = {
        "Größe 'shared_member_gradings'": get_table_size(conn, "shared_member_gradings"),
        "Größe 'shared_grading_feedback'": get_table_size(conn, "shared_grading_feedback"),
        "Gradings ohne Feedback (werden NULL)": get_lost_rows_count(conn, "shared_member_gradings", "shared_grading_feedback", "id", "grading_id")
    }

    query = """
    SELECT 
        m.id AS member_id,
        m.name,
        m.subject_id,
        a.id AS answer_id,
        a.question_id,
        a.content AS answer,
        a.question,
        g.id AS grading_id,
        g.accuracy,
        g.content AS model_prediction,
        g.used_model,
        g.used_rubric,
        g.used_examples,
        g.response AS model_response_with_metadata,
        f.rating,
        f.comment
    FROM shared_members m
    JOIN shared_member_answers a ON m.id = a.shared_member_id
    JOIN shared_member_gradings g ON a.id = g.answer_id
    LEFT JOIN shared_grading_feedback f ON g.id = f.grading_id
    """

    df = pd.read_sql_query(query, conn)

    return df, stats

def main():

    if not os.path.exists('raw_data'):
        os.makedirs('raw_data')
        print("Ordner 'raw_data' wurde erstellt.")

    try_anonymize_data();

    conn = None
    try:
        conn = open_connection()
        
        # join one
        p_name = 'join_one'
        df, stats = join_one(conn)
        
        fallback(p_name, df, stats)

        # join two
        p_name = 'join_two'
        df, stats = join_two(conn)
        
        fallback(p_name, df, stats)

        # join three
        p_name = 'join_three'
        df, stats = join_three(conn)
        
        fallback(p_name, df, stats)


    finally:
        if conn:
            close_connection(conn)

main()

print("Fertig! Die Parquet-Datei wurde erstellt.")